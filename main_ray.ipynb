{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.12 64-bit ('obj_detect': conda)",
   "display_name": "Python 3.6.12 64-bit ('obj_detect': conda)",
   "metadata": {
    "interpreter": {
     "hash": "7deb1b2438bfe545aae19c35ee68784cd1c44f3434dac092ec0b61f0646d1489"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import time as tt\n",
    "import datetime\n",
    "import torch\n",
    "import numpy as np\n",
    "from PPO.Agent import ppoAgent\n",
    "from PPO.wrapper import preprocessBatch\n",
    "from baseline.utils import jsonParser, getOptim\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "from mlagents_envs.side_channel.environment_parameters_channel import EnvironmentParametersChannel\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './cfg/WOImage.json'\n",
    "parser = jsonParser(path)\n",
    "data = parser.loadParser()\n",
    "aData = parser.loadAgentParser()\n",
    "optimData = parser.loadOptParser()\n",
    "device = data['device']\n",
    "writeMode = data['writeTMode']\n",
    "tPath = data['tPath']\n",
    "lPath = data['lPath']\n",
    "sPath = data['sPath']\n",
    "k1 = data['K1']\n",
    "k2 = data['K2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nEnv = data['nEnv']\n",
    "nAgent = 8\n",
    "TotalAgent = nEnv * nAgent\n",
    "ReplayMemory = deque(maxlen=int(1e5))\n",
    "ReplayMemory_Trajectory = deque(maxlen=int(1e5))\n",
    "step = 0\n",
    "ClipingNormCritic = 100000\n",
    "ClipingNormActor = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Ray and Specify default data type of torch.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-02-10 16:59:39,960\tINFO services.py:1173 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "ray.init(num_cpus=8)\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load hyper-Parameter for Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropyCoeff = data['entropyCoeff']\n",
    "epsilon = data['epsilon']\n",
    "lambda_ = data['lambda']\n",
    "initLogStd = torch.tensor(data['initLogStd']).to(device)\n",
    "finLogStd = torch.tensor(data['finLogStd']).to(device)\n",
    "annealingStep = data['annealingStep']\n",
    "LSTMName = data['LSTMName']\n",
    "sSize = data['sSize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = data['gamma']\n",
    "epoch = data['epoch']\n",
    "updateOldP = data['updateOldP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pureEnv = data['envName'].split('/')\n",
    "name = pureEnv[-1]\n",
    "time = datetime.datetime.now().strftime(\"%Y%m%d-%H-%M-%S\")\n",
    "if writeMode:\n",
    "    tPath = tPath + name + time\n",
    "    writer = SummaryWriter(tPath)\n",
    "sPath += name + '_' + str(time) +'.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = \\\n",
    "    \"\"\"\n",
    "    Configuration for this experiment\n",
    "    \"\"\"\n",
    "def writeDict(_data, key, n=0):\n",
    "    global info\n",
    "    tab = \"\"\n",
    "    for _ in range(n):\n",
    "        tab += '\\t'\n",
    "    if type(_data) == dict:\n",
    "        for k in _data.keys():\n",
    "            dK = _data[k]\n",
    "            if type(dK) == dict:\n",
    "                info +=\\\n",
    "            \"\"\"\n",
    "        {}{}:\n",
    "            \"\"\".format(tab, k)\n",
    "                writeDict(dK, k, n=n+1)\n",
    "            else:\n",
    "                info += \\\n",
    "        \"\"\"\n",
    "        {}{}:{}\n",
    "        \"\"\".format(tab, k, dK)\n",
    "    else:\n",
    "        info +=\\\n",
    "        \"\"\"\n",
    "        {}:{}\n",
    "        \"\"\".format(key, _data)\n",
    "\n",
    "def writeTrainInfo():\n",
    "    global info\n",
    "    key = data.keys()\n",
    "    for k in key:\n",
    "        _data = data[k]\n",
    "        if type(_data) == dict:\n",
    "            info +=\\\n",
    "        \"\"\"\n",
    "        {}:\n",
    "        \"\"\".format(k)\n",
    "            writeDict(_data, k ,n=1)\n",
    "        else:\n",
    "            writeDict(_data, k)\n",
    "    print(info)\n",
    "    if writeMode:\n",
    "        writer.add_text('Information', info, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n    Configuration for this experiment\n    \n        sSize:[1, 360]\n        \n        aSize:2\n        \n        envName:./Env/Linux/Cargo_8_easy\n        \n        nEnv:4\n        \n        time_scale:1\n        \n        RecordScore:2000\n        \n        no_graphics:False\n        \n        env:\n        \n        \tNumber_Agent:32\n        \n        \tMaxStep:961\n        \n        \tInterval_Decision:5\n        \n        \tInterval_Update_Position:2\n        \n        \tReward_ReachingDestination:2.3\n        \n        \tReward_ArrivingDestination:50\n        \n        \tPenalty_Collision_Wall:-0.1\n        \n        \tPenalty_Collision_Agent:-0.5\n        \n        \tPenalty_Collision_DynamicOBS:0\n        \n        LSTMName:module05\n        \n        agent:\n        \n        \tactor:\n            \n        \t\tmodule00:\n            \n        \t\t\tnetCat:Cat\n        \n        \t\t\tinput:[0]\n        \n        \t\t\tprior:1\n        \n        \t\t\tprevNodeNames:['module01']\n        \n        \t\tmodule01:\n            \n        \t\t\tnetCat:CNN1D\n        \n        \t\t\tiSize:1\n        \n        \t\t\tnLayer:3\n        \n        \t\t\tnUnit:[4, 8, 16]\n        \n        \t\t\tfSize:[5, 3, 3, -1]\n        \n        \t\t\tpadding:[0, 0, 0]\n        \n        \t\t\tstride:[3, 2, 2]\n        \n        \t\t\tact:['relu', 'relu', 'relu']\n        \n        \t\t\tlinear:True\n        \n        \t\t\tinput:[1]\n        \n        \t\t\tprior:0\n        \n        \t\tmodule04:\n            \n        \t\t\tnetCat:View\n        \n        \t\t\tshape:[-1, 32, 472]\n        \n        \t\t\tprior:2\n        \n        \t\t\tprevNodeNames:['module00']\n        \n        \t\tmodule05:\n            \n        \t\t\tnetCat:LSTMNET\n        \n        \t\t\tiSize:472\n        \n        \t\t\tnLayer:1\n        \n        \t\t\thiddenSize:256\n        \n        \t\t\tNumber_Agent:32\n        \n        \t\t\tFlattenMode:True\n        \n        \t\t\tdevice:cuda:0\n        \n        \t\t\tprior:3\n        \n        \t\t\tprevNodeNames:['module04']\n        \n        \t\tmodule06:\n            \n        \t\t\tnetCat:MLP\n        \n        \t\t\tiSize:256\n        \n        \t\t\tnLayer:3\n        \n        \t\t\tfSize:[256, 256, 3]\n        \n        \t\t\tact:['relu', 'relu', 'linear']\n        \n        \t\t\tBN:False\n        \n        \t\t\toutput:True\n        \n        \t\t\tprevNodeNames:['module05']\n        \n        \t\t\tprior:4\n        \n        \tsSize:[1, 360]\n        \n        \taSize:2\n        \n        \tdevice:cuda:0\n        \n        \tgamma:0.99\n        \n        gamma:0.99\n        \n        lambda:0.95\n        \n        rScaling:1\n        \n        entropyCoeff:0\n        \n        epsilon:0.2\n        \n        div:1\n        \n        epoch:8\n        \n        updateOldP:8\n        \n        initLogStd:-0.6\n        \n        finLogStd:-1.3\n        \n        annealingStep:1000000.0\n        \n        K1:160\n        \n        K2:10\n        \n        optim:\n        \n        \tactor:\n            \n        \t\tname:adam\n        \n        \t\tlr:0.0003\n        \n        \t\tdecay:1e-05\n        \n        \t\teps:1e-07\n        \n        \t\tclipping:False\n        \n        sPath:./save/PPO/\n        \n        writeTMode:True\n        \n        tPath:./tensorboard/PPO/\n        \n        lPath:None\n        \n        device:cuda:0\n        \n        gpuOverload:False\n        \n        inferMode:False\n        \n        renderMode:True\n        \n"
     ]
    }
   ],
   "source": [
    "writeTrainInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instances for Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agent = ppoAgent(\n",
    "    aData,\n",
    "    coeff=entropyCoeff,\n",
    "    epsilon=epsilon,\n",
    "    device=device,\n",
    "    initLogStd=initLogStd,\n",
    "    finLogStd=finLogStd,\n",
    "    annealingStep=annealingStep,\n",
    "    LSTMName=LSTMName\n",
    ")\n",
    "\n",
    "if lPath != \"None\":\n",
    "    Agent.load_state_dict(\n",
    "        torch.load(lPath, map_location=device)\n",
    "    )\n",
    "    Agent.loadParameters()\n",
    "\n",
    "OldAgent = ppoAgent(\n",
    "    aData,\n",
    "    coeff=entropyCoeff,\n",
    "    epsilon=epsilon,\n",
    "    device=device,\n",
    "    initLogStd=initLogStd,\n",
    "    finLogStd=finLogStd,\n",
    "    annealingStep=annealingStep,\n",
    "    LSTMName=LSTMName\n",
    ")\n",
    "OldAgent.update(Agent)\n",
    "\n",
    "CopyAgent = ppoAgent(\n",
    "    aData,\n",
    "    coeff=entropyCoeff,\n",
    "    epsilon=epsilon,\n",
    "    device=device,\n",
    "    initLogStd=initLogStd,\n",
    "    finLogStd=finLogStd,\n",
    "    annealingStep=annealingStep,\n",
    "    LSTMName=LSTMName\n",
    ")\n",
    "CopyAgent.update(Agent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration for Unity Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2m\u001b[36m(pid=6330)\u001b[0m Found path: /home/seungju/LSTMSAC/./Env/Linux/Cargo_8_easy.x86_64\n",
      "\u001b[2m\u001b[36m(pid=6334)\u001b[0m Found path: /home/seungju/LSTMSAC/./Env/Linux/Cargo_8_easy.x86_64\n",
      "\u001b[2m\u001b[36m(pid=6328)\u001b[0m Found path: /home/seungju/LSTMSAC/./Env/Linux/Cargo_8_easy.x86_64\n",
      "\u001b[2m\u001b[36m(pid=6333)\u001b[0m Found path: /home/seungju/LSTMSAC/./Env/Linux/Cargo_8_easy.x86_64\n",
      "\n",
      "Load the Unity Environment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_id = 32\n",
    "time_scale = data['time_scale']\n",
    "envData = data['env']\n",
    "no_graphics = data['no_graphics']\n",
    "\n",
    "\n",
    "engineChannel = EngineConfigurationChannel()\n",
    "engineChannel.set_configuration_parameters(time_scale=time_scale)\n",
    "setChannel = EnvironmentParametersChannel()\n",
    "for key in envData.keys():\n",
    "    setChannel.set_float_parameter(key, float(envData[key]))\n",
    "name = data['envName']\n",
    "envs = []\n",
    "for i in range(nEnv):\n",
    "    env = ray.remote(num_cpus=1)(UnityEnvironment)\n",
    "    ENV = env.remote(\n",
    "        name,\n",
    "        worker_id=_id+i,\n",
    "        side_channels=[setChannel, engineChannel],\n",
    "        no_graphics=no_graphics,\n",
    "        seed = 1 + i * _id\n",
    "    )\n",
    "    ENV.reset.remote()\n",
    "    envs.append(ENV)\n",
    "\n",
    "behaviorNames = 'Agent?team=0'\n",
    "for e in envs:\n",
    "    ray.get(e._assert_behavior_exists.remote(behaviorNames))\n",
    "\n",
    "print(\"\"\"\n",
    "Load the Unity Environment\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling and Training, AND Sampling,...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def _getObs(env, behaviorNames, nAgent, init):\n",
    "    decisionStep, terminalStep = ray.get(env.get_steps.remote(behaviorNames))\n",
    "    image = decisionStep.obs[0]\n",
    "    obs = decisionStep.obs[1]\n",
    "    rewards = decisionStep.reward\n",
    "    obs = obs.tolist()\n",
    "\n",
    "    obs = list(map(lambda x: np.array(x), obs))\n",
    "    obs = np.array(obs)\n",
    "\n",
    "    done = []\n",
    "    \n",
    "    for done_idx in obs[:, -1]:\n",
    "        done.append(done_idx == 1)\n",
    "    reward = rewards\n",
    "    \n",
    "    obsState = (obs, image)\n",
    "\n",
    "    if init:\n",
    "        return obsState\n",
    "    else:\n",
    "        return(obsState, reward, done)\n",
    "\n",
    "def getObs(init=False) -> tuple:\n",
    "    done = [False for i in range(TotalAgent)]\n",
    "    reward = [0 for i in range(TotalAgent)]\n",
    "    proc = []\n",
    "    vectorObs, imageObs = np.zeros((TotalAgent, 369)), np.zeros((TotalAgent, 96, 96, 1))\n",
    "    for i in range(nEnv):\n",
    "        proc.append(_getObs.remote(\n",
    "            envs[i],\n",
    "            behaviorNames,\n",
    "            nAgent,\n",
    "            init\n",
    "        ))\n",
    "    for i in range(nEnv):\n",
    "        t = ray.get(proc[i])\n",
    "        if init:\n",
    "            s = t\n",
    "            vectorObs[i*nAgent:(i+1)*nAgent] = s[0]\n",
    "            imageObs[i*nAgent:(i+1)*nAgent] = s[1]\n",
    "        else:\n",
    "            s, r, d = t\n",
    "            vectorObs[i*nAgent:(i+1)*nAgent] = s[0]\n",
    "            imageObs[i*nAgent:(i+1)*nAgent] = s[1]\n",
    "            done[i*nAgent:(i+1)*nAgent] = d\n",
    "            reward[i*nAgent:(i+1)*nAgent] = r\n",
    "    \n",
    "    obsState = (vectorObs, imageObs)\n",
    "    if init:\n",
    "        return obsState\n",
    "    else:\n",
    "        return (obsState, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppState(obs) -> tuple:\n",
    "    vectorObs, imageObs = obs\n",
    "    rState = torch.tensor(vectorObs[:, :8]).to(device).double()\n",
    "    lidarPt = torch.tensor(vectorObs[:, 8:-1]).to(device).double()\n",
    "    lidarPt = torch.unsqueeze(lidarPt, dim=1)\n",
    "    state = (rState, lidarPt)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAction(state) -> np.ndarray:\n",
    "    with torch.no_grad():\n",
    "        action = OldAgent.actorForward(state)\n",
    "        action = action.cpu().numpy()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkStep(action) -> None:\n",
    "    for i in range(nEnv):\n",
    "        act = action[i*nAgent:(i+1)*nAgent]\n",
    "        envs[i].set_actions.remote(\n",
    "            behaviorNames,\n",
    "            act\n",
    "        )\n",
    "        envs[i].step.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rewards = np.zeros(TotalAgent)\n",
    "episodeReward = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateOptim() -> tuple:\n",
    "    optimKeyList = list(optimData.keys())\n",
    "    for key in optimKeyList:\n",
    "        if key == \"actor\":\n",
    "            aOptim = getOptim(\n",
    "                optimData[key],\n",
    "                Agent.actor.buildOptim())\n",
    "    return aOptim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "aOptim = GenerateOptim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Set Zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroGrad() -> None:\n",
    "    aOptim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroGrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    PPOAGENT,\n",
    "    state,\n",
    "    action,\n",
    "    gT,\n",
    "    gAE,\n",
    "    critic,\n",
    "    _step,\n",
    "    _epoch\n",
    "):\n",
    "    PPOAGENT:ppoAgent\n",
    "    lossC, minusObj, entropy = PPOAGENT.calLoss(\n",
    "            CopyAgent,\n",
    "            state,\n",
    "            action.detach(),\n",
    "            gT.detach(),\n",
    "            critic.detach(),\n",
    "            gAE.detach()\n",
    "        )\n",
    "\n",
    "    objectFunction = minusObj + lossC\n",
    "    obj = minusObj.cpu().sum().detach().numpy()\n",
    "    lossC = lossC.cpu().sum().detach().numpy()\n",
    "\n",
    "    if writeMode:\n",
    "        \n",
    "        writer.add_scalar(\"Obj\", -obj, _step+_epoch)\n",
    "        writer.add_scalar(\"Critic Lostt\", lossC, _step+_epoch)\n",
    "        entropy = entropy.detach().cpu().numpy()\n",
    "        writer.add_scalar(\"Entropy\", entropy, _step + _epoch)\n",
    "\n",
    "        gT = gT.view(-1)\n",
    "        gT = torch.mean(gT).detach().cpu().numpy()\n",
    "        writer.add_scalar(\"gT\", gT, _step + _epoch)\n",
    "\n",
    "        critic = critic.view(-1)\n",
    "        critic = torch.mean(critic).detach().cpu().numpy()\n",
    "        writer.add_scalar(\"critic\", critic, _step + _epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReturn(\n",
    "    reward,\n",
    "    critic,\n",
    "    nCritic,\n",
    "    done\n",
    ")->tuple:\n",
    "    gT, gAE = [], []\n",
    "    step = len(reward)\n",
    "    critic = critic.view((step, -1))\n",
    "    nCritic = nCritic.view((step, -1))\n",
    "    for i in range(nAgent):\n",
    "        rA = reward[:, i]  # 160\n",
    "        dA = done[:, i]  # 160\n",
    "        cA = critic[:, i] \n",
    "        ncA = nCritic[:, i] \n",
    "        GT = []\n",
    "        GTDE = []\n",
    "        discounted_Td = 0\n",
    "        discounted_r = ncA[-1]\n",
    "\n",
    "        for r, is_terminal, c, nc in zip(\n",
    "                reversed(rA), \n",
    "                reversed(dA), \n",
    "                reversed(cA),\n",
    "                reversed(ncA)):\n",
    "            td_error = r + gamma * nc - c\n",
    "            discounted_r = r + gamma * discounted_r\n",
    "            discounted_Td = td_error + gamma * lambda_ * discounted_Td\n",
    "            GT.append(discounted_r)\n",
    "            GTDE.append(discounted_Td)\n",
    "        GT = torch.tensor(GT[::-1]).view((-1, 1)).to(device)\n",
    "        GTDE = torch.tensor(GTDE[::-1]).view((-1, 1)).to(device)\n",
    "        gT.append(GT)\n",
    "        gAE.append(GTDE)\n",
    "\n",
    "    gT = torch.cat(gT, dim=0)\n",
    "    gAE = torch.cat(gAE, dim=0)\n",
    "\n",
    "    gT = gT.view(nAgent, -1)\n",
    "    gT = gT.permute(1, 0).contiguous()\n",
    "    gT = gT.view((-1, 1))\n",
    "\n",
    "    gAE = gAE.view(nAgent, -1)\n",
    "    gAE = gAE.permute(1, 0).contiguous()\n",
    "    gAE = gAE.view((-1, 1))\n",
    "\n",
    "    return gT, gAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepGradient(_step, _epoch):\n",
    "    Agent.actor.clippingNorm(ClipingNormActor)\n",
    "    aOptim.step()\n",
    "\n",
    "    if writeMode:\n",
    "        \n",
    "        writer.add_scalar('Action Gradient Mag', normA, _step+_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessBatch(_step, _epoch):\n",
    "    div = int(k1/k2)\n",
    "\n",
    "    # Ready for the batch-preprocessing\n",
    "    rstate, lidarPt, action, reward, done = \\\n",
    "        [], [], [], [], []\n",
    "    num_list = int(len(ReplayMemory_Trajectory)/k1)\n",
    "    trstate, tlidarPt =\\\n",
    "        [[] for __ in range(num_list)],\\\n",
    "        [[] for __ in range(num_list)]\n",
    "    tState = [[] for _ in range(num_list - 1)]\n",
    "\n",
    "    # get the samples from the replayMemory\n",
    "    for data in ReplayMemory:\n",
    "        s, a, r, ns, d = data\n",
    "        rstate.append(s[0])\n",
    "        lidarPt.append(s[1])\n",
    "        action.append(a)\n",
    "        reward.append(r)\n",
    "        done.append(d)\n",
    "    \n",
    "    # z can be thought as the number for slicing the trajectory value\n",
    "    # by slicing the trajectory samples, reduce the memory usage.\n",
    "    z = 0\n",
    "    for data in ReplayMemory_Trajectory:\n",
    "        \n",
    "        ts = data\n",
    "        trstate[int(z/k1)].append(ts[0])\n",
    "        tlidarPt[int(z/k1)].append(ts[1])\n",
    "        z += 1\n",
    "    \n",
    "    # First K1 Horizon, there is no need to prepare the trajectory.\n",
    "    if len(trstate) == k1:\n",
    "        zeroMode = True\n",
    "    else:\n",
    "        for _ in range(num_list - 1):\n",
    "            tState[_] = (\n",
    "                torch.cat(trstate[_], dim=0),\n",
    "                torch.cat(tlidarPt[_], dim=0))\n",
    "        zeroMode = False\n",
    "    \n",
    "    # Second preprocess-batch\n",
    "    rstate = torch.cat(rstate, dim=0)\n",
    "    lidarPt = torch.cat(lidarPt, dim=0)\n",
    "    nrstate, nlidarPt = ns\n",
    "\n",
    "    # nrstate, nlidarPt have K1+1 elements\n",
    "    nrstate, nlidarPt =\\\n",
    "        torch.cat((rstate, nrstate), dim=0),\\\n",
    "        torch.cat((lidarPt, nlidarPt), dim=0)\n",
    "    nstate = (nrstate, nlidarPt)\n",
    "\n",
    "    # viewing the tensor, sequence, nAgent, data\n",
    "    # this form for BPTT.\n",
    "    lidarPt = lidarPt.view((-1, TotalAgent, 1, 360))\n",
    "    rstate = rstate.view((-1, TotalAgent, 8))\n",
    "   \n",
    "    # data casting.\n",
    "    reward = np.array(reward)\n",
    "    done = np.array(done)\n",
    "    action = torch.tensor(action).to(device)\n",
    "\n",
    "    # initalize the cell state of agent at the 0 step.\n",
    "    Agent.actor.zeroCellState()\n",
    "    CopyAgent.actor.zeroCellState()\n",
    "\n",
    "    # 0. get the cell state before the K1 Step.\n",
    "    # To do this, we use trajectory samples by just forwarding them.\n",
    "    if zeroMode is False:\n",
    "        with torch.no_grad():\n",
    "            for tr in tState:\n",
    "                tr_cuda = tuple([x.to(device) for x in tr])\n",
    "                Agent.actor.forward(tr_cuda)\n",
    "                CopyAgent.actor.forward(tr_cuda)\n",
    "                del tr_cuda\n",
    "            # detaching!!\n",
    "            Agent.actor.detachCellState()\n",
    "            CopyAgent.actor.detachCellState()\n",
    "\n",
    "    Agent.actor.detachCellState()\n",
    "    InitActorCellState = Agent.actor.getCellState()\n",
    "    InitCopyActorCellState = CopyAgent.actor.getCellState()\n",
    "    zeroGrad()\n",
    "\n",
    "    # 2. implemented the training using the truncated BPTT\n",
    "    for _ in range(epoch):\n",
    "        # reset the agent at previous K1 step.\n",
    "        Agent.actor.setCellState(InitActorCellState)\n",
    "\n",
    "        # by this command, cell state of agent reaches the current Step.\n",
    "        value = Agent.criticForward(nstate)\n",
    "        \n",
    "        # calculate the target value for training\n",
    "        value = value.view(k1+1, TotalAgent, 1)\n",
    "        nvalue = value[1:]\n",
    "        value = value[:-1]\n",
    "        gT, gAE = getReturn(reward, value, nvalue, done)\n",
    "        gT = gT.view(k1, TotalAgent)\n",
    "        gAE = gAE.view(k1, TotalAgent)\n",
    "\n",
    "        # before training, reset the cell state of agent at Previous K1 step.\n",
    "        CopyAgent.actor.setCellState(InitCopyActorCellState)\n",
    "        Agent.actor.setCellState(InitActorCellState)\n",
    "        \n",
    "        # div can be thought as slice size for BPTT.\n",
    "        for i in range(div):\n",
    "            # ready for the batching.\n",
    "            _rstate = rstate[i*k2:(i+1)*k2].view(-1, 8).detach()\n",
    "            _lidarpt = lidarPt[i*k2:(1+i)*k2].view(-1, 1, sSize[-1]).detach()\n",
    "            _state = (_rstate, _lidarpt)\n",
    "            _action = action[i*k2:(i+1)*k2].view((-1, 2))\n",
    "            _gT = gT[i*k2:(i+1)*k2].view(-1, 1)\n",
    "            _gAE = gAE[i*k2:(i+1)*k2].view(-1, 1)\n",
    "            _value = value[i*k2:(i+1)*k2].view(-1, 1)\n",
    "\n",
    "            # after calling f, cell state would jump K2 Step from the previous Step.\n",
    "            train(Agent, _state, _action, _gT, _gAE, _value, step, epoch)\n",
    "\n",
    "            # detaching device for BPTT\n",
    "            Agent.actor.detachCellState()\n",
    "        \n",
    "        # step the gradient for updating\n",
    "        stepGradient(step+i, epoch)\n",
    "        zeroGrad()\n",
    "\n",
    "        # get the new cell state of new agent\n",
    "        # Initialize the agent at 0 step.\n",
    "        Agent.actor.zeroCellState()\n",
    "        if zeroMode is False:\n",
    "            with torch.no_grad():\n",
    "                for tr in tState:\n",
    "                    tr_cuda = tuple([x.to(device) for x in tr])\n",
    "                    Agent.actor.forward(tr_cuda)\n",
    "                    del tr_cuda\n",
    "                Agent.actor.detachCellState()\n",
    "        InitActorCellState = Agent.actor.getCellState()\n",
    "    \n",
    "    del tState,  InitActorCellState,  \\\n",
    "        InitCopyActorCellState\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "shape '[160, 32]' is invalid for input of size 1280",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-f533e84f687f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'K1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mpreprocessBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mReplayMemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-0ed30095300d>\u001b[0m in \u001b[0;36mpreprocessBatch\u001b[0;34m(_step, _epoch)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mgT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgAE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetReturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mgT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTotalAgent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mgAE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTotalAgent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[160, 32]' is invalid for input of size 1280"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "episodeReward = []\n",
    "k = 0\n",
    "Rewards = np.zeros(TotalAgent)\n",
    "obs = getObs(init=True)\n",
    "stateT = ppState(obs)\n",
    "action = getAction(stateT)\n",
    "while 1:\n",
    "    checkStep(action)\n",
    "\n",
    "    obs, reward, done = getObs()\n",
    "\n",
    "    Rewards += reward\n",
    "\n",
    "    nStateT = ppState(obs)\n",
    "    nAction = getAction(nStateT)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ReplayMemory.append(\n",
    "            (\n",
    "                stateT,\n",
    "                action.copy(),\n",
    "                reward.copy(),\n",
    "                nStateT,\n",
    "                done\n",
    "            )\n",
    "        )\n",
    "        stateT_cpu = tuple([x.cpu() for x in stateT])\n",
    "        ReplayMemory_Trajectory.append(\n",
    "            stateT_cpu\n",
    "        )\n",
    "    \n",
    "    action = nAction\n",
    "    stateT = nStateT\n",
    "    step += 1\n",
    "\n",
    "    Agent.decayingLogStd(step)\n",
    "    CopyAgent.decayingLogStd(step)\n",
    "    OldAgent.decayingLogStd(step)\n",
    "\n",
    "    if (step) % (data['K1']) == 0:\n",
    "        k += 1\n",
    "        preprocessBatch(step, epoch)\n",
    "        ReplayMemory.clear()\n",
    "\n",
    "        if k % updateOldP == 0:\n",
    "            OldAgent.update(Agent)\n",
    "            CopyAgent.update(Agent)\n",
    "            k = 0\n",
    "    \n",
    "    if True in done:\n",
    "        Agent.actor.zeroCellState()\n",
    "        OldAgent.actor.zeroCellState()\n",
    "        CopyAgent.actor.zeroCellState()\n",
    "        ReplayMemory_Trajectory.clear()\n",
    "        for env in envs:\n",
    "            env.step.remote()\n",
    "        \n",
    "        obs = getObs(init=True)\n",
    "        stateT = ppState(obs)\n",
    "        action = getAction(stateT)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}