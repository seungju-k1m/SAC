{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.12 64-bit ('obj_detect': conda)",
   "display_name": "Python 3.6.12 64-bit ('obj_detect': conda)",
   "metadata": {
    "interpreter": {
     "hash": "7deb1b2438bfe545aae19c35ee68784cd1c44f3434dac092ec0b61f0646d1489"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# export DISPLAY=:0\n",
    "# xhost +si:localuser:root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ray\n",
    "import time as tt\n",
    "import datetime\n",
    "import torch\n",
    "import numpy as np\n",
    "from PPO.Agent import ppoAgent\n",
    "from PPO.wrapper import preprocessBatch\n",
    "from baseline.utils import jsonParser, getOptim\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "from mlagents_envs.side_channel.environment_parameters_channel import EnvironmentParametersChannel\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './cfg/LSTMTrain.json'\n",
    "parser = jsonParser(path)\n",
    "data = parser.loadParser()\n",
    "aData = parser.loadAgentParser()\n",
    "optimData = parser.loadOptParser()\n",
    "device = data['device']\n",
    "writeMode = data['writeTMode']\n",
    "tPath = data['tPath']\n",
    "lPath = data['lPath']\n",
    "sPath = data['sPath']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nEnv = data['nEnv']\n",
    "nAgent = 64\n",
    "TotalAgent = nEnv * nAgent\n",
    "ReplayMemory = deque(maxlen=int(1e5))\n",
    "ReplayMemory_Trajectory = deque(maxlen=int(1e5))\n",
    "step = 0\n",
    "ClipingNormCritic = 10\n",
    "ClipingNormActor = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Ray and Specify default data type of torch.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-01-19 21:33:32,671\tINFO services.py:1173 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "ray.init(num_cpus=8)\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load hyper-Parameter for Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropyCoeff = data['entropyCoeff']\n",
    "epsilon = data['epsilon']\n",
    "lambda_ = data['lambda']\n",
    "initLogStd = torch.tensor(data['initLogStd']).to(device)\n",
    "finLogStd = torch.tensor(data['finLogStd']).to(device)\n",
    "annealingStep = data['annealingStep']\n",
    "LSTMNum = data['LSTMNum']\n",
    "sSize = data['sSize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = data['gamma']\n",
    "epoch = data['epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pureEnv = data['envName'].split('/')\n",
    "name = pureEnv[-1]\n",
    "time = datetime.datetime.now().strftime(\"%Y%m%d-%H-%M-%S\")\n",
    "if writeMode:\n",
    "    tPath = tPath + name + time\n",
    "    writer = SummaryWriter(tPath)\n",
    "sPath += name + '_' + str(time) +'.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = \\\n",
    "    \"\"\"\n",
    "    Configuration for this experiment\n",
    "    \"\"\"\n",
    "def writeDict(_data, key, n=0):\n",
    "    global info\n",
    "    tab = \"\"\n",
    "    for _ in range(n):\n",
    "        tab += '\\t'\n",
    "    if type(_data) == dict:\n",
    "        for k in _data.keys():\n",
    "            dK = _data[k]\n",
    "            if type(dK) == dict:\n",
    "                info +=\\\n",
    "            \"\"\"\n",
    "        {}{}:\n",
    "            \"\"\".format(tab, k)\n",
    "                writeDict(dK, k, n=n+1)\n",
    "            else:\n",
    "                info += \\\n",
    "        \"\"\"\n",
    "        {}{}:{}\n",
    "        \"\"\".format(tab, k, dK)\n",
    "    else:\n",
    "        info +=\\\n",
    "        \"\"\"\n",
    "        {}:{}\n",
    "        \"\"\".format(key, _data)\n",
    "\n",
    "def writeTrainInfo():\n",
    "    global info\n",
    "    key = data.keys()\n",
    "    for k in key:\n",
    "        _data = data[k]\n",
    "        if type(_data) == dict:\n",
    "            info +=\\\n",
    "        \"\"\"\n",
    "        {}:\n",
    "        \"\"\".format(k)\n",
    "            writeDict(_data, k ,n=1)\n",
    "        else:\n",
    "            writeDict(_data, k)\n",
    "    print(info)\n",
    "    if writeMode:\n",
    "        writer.add_text('Information', info, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n    Configuration for this experiment\n    \n        sSize:[1, 360]\n        \n        aSize:2\n        \n        envName:./Env/Linux/Lotte08\n        \n        time_scale:3\n        \n        RecordScore:2000\n        \n        no_graphics:False\n        \n        nEnv:4\n        \n        env:\n        \n        \tLSTMMode:0\n        \n        \tImgMode:False\n        \n        \tResolution_LidarAngle:1\n        \n        \tNumber_Agent:256\n        \n        \tNumber_MaxCollision:1000000.0\n        \n        \tMaxStep:3201\n        \n        \tPenallty_Maginitude_AngularVelocity:0\n        \n        \tRecommendedYawRate:0.7\n        \n        \tVelocity:4\n        \n        \tYawRate:1\n        \n        \tPenalty_OverAngularVelocity:0\n        \n        \tReward_ReachingDestination:0.02\n        \n        \tPenalty_Distance:0\n        \n        \tReward_ArrivingDestination:15\n        \n        \tReward_ArrivingStartPoint:15\n        \n        \tPenalty_OverMaxStep:0\n        \n        \tPenalty_Time:0\n        \n        \tPenalty_Stay:-0.03\n        \n        \tPenalty_Collision_Wall:-0.2\n        \n        \tPenalty_Collision_Agent:-0.5\n        \n        \tPenalty_Collision_DynamicOBS:-0.3\n        \n        \tNumber_MaxArrivingReward:1000000.0\n        \n        \tIntervalStep:5\n        \n        \tLidarMeasurementDistance:10\n        \n        \tSwitchDistance:1\n        \n        LSTMNum:3\n        \n        agent:\n        \n        \tactor:\n            \n        \t\tmodule00:\n            \n        \t\t\tnetCat:CNN1D\n        \n        \t\t\tiSize:1\n        \n        \t\t\tnLayer:3\n        \n        \t\t\tnUnit:[16, 8, 4]\n        \n        \t\t\tfSize:[5, 3, 3, -1]\n        \n        \t\t\tpadding:[0, 0, 0]\n        \n        \t\t\tstride:[3, 2, 2]\n        \n        \t\t\tact:['relu', 'relu', 'relu']\n        \n        \t\t\tlinear:True\n        \n        \t\t\tinput:[1]\n        \n        \t\tmodule01:\n            \n        \t\t\tnetCat:Cat\n        \n        \t\t\tinput:[0]\n        \n        \t\tmodule02:\n            \n        \t\t\tnetCat:View\n        \n        \t\t\tshape:[-1, 256, 122]\n        \n        \t\tmodule03:\n            \n        \t\t\tnetCat:LSTMNET\n        \n        \t\t\tiSize:122\n        \n        \t\t\tnLayer:1\n        \n        \t\t\thiddenSize:256\n        \n        \t\t\tNumber_Agent:256\n        \n        \t\t\tFlattenMode:True\n        \n        \t\t\tdevice:cuda:0\n        \n        \t\tmodule04:\n            \n        \t\t\tnetCat:MLP\n        \n        \t\t\tiSize:256\n        \n        \t\t\tnLayer:3\n        \n        \t\t\tfSize:[256, 256, 2]\n        \n        \t\t\tact:['relu', 'relu', 'linear']\n        \n        \t\t\tBN:False\n        \n        \t\t\toutput:True\n        \n        \tcritic:\n            \n        \t\tmodule00:\n            \n        \t\t\tnetCat:CNN1D\n        \n        \t\t\tiSize:1\n        \n        \t\t\tnLayer:3\n        \n        \t\t\tnUnit:[16, 8, 4]\n        \n        \t\t\tfSize:[5, 3, 3, -1]\n        \n        \t\t\tpadding:[0, 0, 0]\n        \n        \t\t\tstride:[3, 2, 2]\n        \n        \t\t\tact:['relu', 'relu', 'relu']\n        \n        \t\t\tlinear:True\n        \n        \t\t\tinput:[1]\n        \n        \t\tmodule01:\n            \n        \t\t\tnetCat:Cat\n        \n        \t\t\tinput:[0]\n        \n        \t\tmodule02:\n            \n        \t\t\tnetCat:View\n        \n        \t\t\tshape:[-1, 256, 122]\n        \n        \t\tmodule03:\n            \n        \t\t\tnetCat:LSTMNET\n        \n        \t\t\tiSize:122\n        \n        \t\t\tnLayer:1\n        \n        \t\t\thiddenSize:256\n        \n        \t\t\tNumber_Agent:256\n        \n        \t\t\tFlattenMode:True\n        \n        \t\t\tdevice:cuda:0\n        \n        \t\tmodule04:\n            \n        \t\t\tnetCat:MLP\n        \n        \t\t\tiSize:256\n        \n        \t\t\tnLayer:3\n        \n        \t\t\tfSize:[256, 256, 1]\n        \n        \t\t\tact:['relu', 'relu', 'linear']\n        \n        \t\t\tBN:False\n        \n        \t\t\toutput:True\n        \n        \tsSize:[1, 360]\n        \n        \taSize:2\n        \n        \tdevice:cuda:0\n        \n        \tgamma:0.99\n        \n        gamma:0.99\n        \n        lambda:0.95\n        \n        rScaling:1\n        \n        entropyCoeff:0\n        \n        epsilon:0.2\n        \n        updateStep:160\n        \n        div:1\n        \n        epoch:1\n        \n        updateOldP:4\n        \n        initLogStd:-1.1\n        \n        finLogStd:-1.5\n        \n        annealingStep:1000000.0\n        \n        K1:160\n        \n        K2:10\n        \n        optim:\n        \n        \tactor:\n            \n        \t\tname:adam\n        \n        \t\tlr:0.0003\n        \n        \t\tdecay:1e-05\n        \n        \t\teps:1e-07\n        \n        \t\tclipping:False\n        \n        \tcritic:\n            \n        \t\tname:adam\n        \n        \t\tlr:0.0003\n        \n        \t\tdecay:1e-05\n        \n        \t\tclipping:False\n        \n        \t\teps:1e-07\n        \n        sPath:./save/PPO/\n        \n        writeTMode:True\n        \n        tPath:./tensorboard/PPO/\n        \n        lPath:None\n        \n        device:cuda:0\n        \n        gpuOverload:False\n        \n        inferMode:False\n        \n        renderMode:True\n        \n"
     ]
    }
   ],
   "source": [
    "writeTrainInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instances for Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agent = ppoAgent(\n",
    "    aData,\n",
    "    coeff=entropyCoeff,\n",
    "    epsilon=epsilon,\n",
    "    device=device,\n",
    "    initLogStd=initLogStd,\n",
    "    finLogStd=finLogStd,\n",
    "    annealingStep=annealingStep,\n",
    "    LSTMNum=LSTMNum\n",
    ")\n",
    "\n",
    "if lPath != \"None\":\n",
    "    Agent.load_state_dict(\n",
    "        torch.load(lPath, map_location=device)\n",
    "    )\n",
    "    Agent.loadParameters()\n",
    "\n",
    "OldAgent = ppoAgent(\n",
    "    aData,\n",
    "    coeff=entropyCoeff,\n",
    "    epsilon=epsilon,\n",
    "    device=device,\n",
    "    initLogStd=initLogStd,\n",
    "    finLogStd=finLogStd,\n",
    "    annealingStep=annealingStep,\n",
    "    LSTMNum=LSTMNum\n",
    ")\n",
    "OldAgent.update(Agent)\n",
    "\n",
    "CopyAgent = ppoAgent(\n",
    "    aData,\n",
    "    coeff=entropyCoeff,\n",
    "    epsilon=epsilon,\n",
    "    device=device,\n",
    "    initLogStd=initLogStd,\n",
    "    finLogStd=finLogStd,\n",
    "    annealingStep=annealingStep,\n",
    "    LSTMNum=LSTMNum\n",
    ")\n",
    "CopyAgent.update(Agent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration for Unity Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2m\u001b[36m(pid=5528)\u001b[0m Found path: /home/seungju/LSTMSAC/./Env/Linux/Lotte08.x86_64\n",
      "\u001b[2m\u001b[36m(pid=5526)\u001b[0m Found path: /home/seungju/LSTMSAC/./Env/Linux/Lotte08.x86_64\n",
      "\u001b[2m\u001b[36m(pid=5527)\u001b[0m Found path: /home/seungju/LSTMSAC/./Env/Linux/Lotte08.x86_64\n",
      "\u001b[2m\u001b[36m(pid=5529)\u001b[0m Found path: /home/seungju/LSTMSAC/./Env/Linux/Lotte08.x86_64\n",
      "\n",
      "Load the Unity Environment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_id = 32\n",
    "time_scale = data['time_scale']\n",
    "envData = data['env']\n",
    "no_graphics = data['no_graphics']\n",
    "\n",
    "\n",
    "engineChannel = EngineConfigurationChannel()\n",
    "engineChannel.set_configuration_parameters(time_scale=time_scale)\n",
    "setChannel = EnvironmentParametersChannel()\n",
    "for key in envData.keys():\n",
    "    setChannel.set_float_parameter(key, float(envData[key]))\n",
    "name = data['envName']\n",
    "envs = []\n",
    "for i in range(nEnv):\n",
    "    env = ray.remote(num_cpus=1)(UnityEnvironment)\n",
    "    ENV = env.remote(\n",
    "        name,\n",
    "        worker_id=_id+i,\n",
    "        side_channels=[setChannel, engineChannel],\n",
    "        no_graphics=no_graphics,\n",
    "        seed = 1 + i * _id\n",
    "    )\n",
    "    ENV.reset.remote()\n",
    "    envs.append(ENV)\n",
    "\n",
    "behaviorNames = 'Robot?team=0'\n",
    "for e in envs:\n",
    "    ray.get(e._assert_behavior_exists.remote(behaviorNames))\n",
    "\n",
    "print(\"\"\"\n",
    "Load the Unity Environment\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling and Training, AND Sampling,...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def _getObs(env, behaviorNames, nAgent):\n",
    "    done = [False for i in range(nAgent)]\n",
    "    reward = [0 for i in range(nAgent)]\n",
    "    decisionStep, terminalStep = ray.get(env.get_steps.remote(behaviorNames))\n",
    "    obs, tobs = decisionStep.obs[0], terminalStep.obs[0]\n",
    "    \n",
    "    reward_, treward = decisionStep.reward, terminalStep.reward\n",
    "    treward = np.array(treward)\n",
    "    reward = reward_\n",
    "    tAgentId = terminalStep.agent_id\n",
    "    obsState = np.array(obs)\n",
    "    k = 0\n",
    "    for j, state in zip(tAgentId, tobs):\n",
    "        obsState[j] = np.array(state)\n",
    "        done[j] = True\n",
    "        # reward[j] = treward[k]\n",
    "        k += 1\n",
    "    return (obsState, reward, treward, done)\n",
    "\n",
    "def getObs(init=False) -> tuple:\n",
    "    obsState = np.zeros((TotalAgent, 1447), dtype=np.float64)\n",
    "    done = [False for i in range(TotalAgent)]\n",
    "    reward = [0 for i in range(TotalAgent)]\n",
    "    proc = []\n",
    "    for i in range(nEnv):\n",
    "        proc.append(_getObs.remote(\n",
    "            envs[i],\n",
    "            behaviorNames,\n",
    "            nAgent\n",
    "        ))\n",
    "    for i in range(nEnv):\n",
    "        t = ray.get(proc[i])\n",
    "        s, r, r_, d = t\n",
    "        obsState[i*nAgent:(i+1)*nAgent, :] = s\n",
    "        done[i*nAgent:(i+1)*nAgent] = d\n",
    "        if True in d:\n",
    "            reward[i*nAgent:(i+1)*nAgent] = r_\n",
    "        else:\n",
    "            reward[i*nAgent:(i+1)*nAgent] = r\n",
    "    if init:\n",
    "        return obsState\n",
    "    else:\n",
    "        return (obsState, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppState(obs) -> tuple:\n",
    "    rState = torch.tensor(obs[:, :6]).to(device).double()\n",
    "    lidarPt = torch.tensor(obs[:, 8:sSize[-1]+8]).to(device)\n",
    "    lidarPt = torch.unsqueeze(lidarPt, dim=1).double()\n",
    "    state = (rState, lidarPt)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAction(state) -> np.ndarray:\n",
    "    with torch.no_grad():\n",
    "        action = OldAgent.actorForward(state)\n",
    "        action = action.cpu().numpy()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkStep(action) -> None:\n",
    "    for i in range(nEnv):\n",
    "        act = ActionTuple(\n",
    "            continuous=action[i*nAgent:(i+1)*nAgent, :]\n",
    "        )\n",
    "        envs[i].set_actions.remote(\n",
    "            behaviorNames,\n",
    "            act\n",
    "        )\n",
    "        envs[i].step.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rewards = np.zeros(TotalAgent)\n",
    "episodeReward = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initSampling() -> tuple:\n",
    "    init_obs = getObs(init=True)\n",
    "    stateT = ppState(init_obs)\n",
    "    action = getAction(stateT)\n",
    "\n",
    "    return (stateT, action)\n",
    "\n",
    "def Sampling(stateT, action) -> list:\n",
    "    global Rewards\n",
    "    global episodeReward\n",
    "    global step\n",
    "    with torch.no_grad():\n",
    "        for i in range(160):\n",
    "            z = tt.time()\n",
    "            checkStep(action)\n",
    "            obs, reward, done = getObs()\n",
    "            nstateT = ppState(obs)\n",
    "            nAction = getAction(nstateT)\n",
    "            ReplayMemory.append(\n",
    "                (\n",
    "                    stateT,\n",
    "                    action.copy(),\n",
    "                    reward,\n",
    "                    nstateT,\n",
    "                    done.copy()\n",
    "                )\n",
    "            )\n",
    "            Rewards += reward\n",
    "\n",
    "            stateT_cpu = tuple([x.cpu() for x in stateT])\n",
    "            ReplayMemory_Trajectory.append(\n",
    "                stateT_cpu\n",
    "            )\n",
    "            action = nAction\n",
    "            stateT = nstateT\n",
    "            step += 1\n",
    "            Agent.decayingLogStd(step)\n",
    "            OldAgent.decayingLogStd(step)\n",
    "            CopyAgent.decayingLogStd(step)\n",
    "            # print(\"InferenceTime:{:.3f}\".format(tt.time() -z))\n",
    "    \n",
    "    return done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateOptim() -> tuple:\n",
    "    optimKeyList = list(optimData.keys())\n",
    "    for key in optimKeyList:\n",
    "        if key == \"actor\":\n",
    "            aOptim = getOptim(\n",
    "                optimData[key],\n",
    "                Agent.actor.buildOptim())\n",
    "        if key == \"critic\":\n",
    "            cOptim = getOptim(\n",
    "                optimData[key],\n",
    "                Agent.critic.buildOptim()\n",
    "            )\n",
    "    return (aOptim, cOptim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "aOptim, cOptim = GenerateOptim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Set Zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroGrad() -> None:\n",
    "    aOptim.zero_grad()\n",
    "    cOptim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroGrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    PPOAGENT,\n",
    "    state,\n",
    "    action,\n",
    "    gT,\n",
    "    gAE,\n",
    "    critic,\n",
    "    _step,\n",
    "    _epoch\n",
    "):\n",
    "    PPOAGENT:ppoAgent\n",
    "    lossC = PPOAGENT.calQLoss(\n",
    "        state,\n",
    "        gT.detach()\n",
    "    )\n",
    "    lossC.backward()\n",
    "    \n",
    "    minusObj, entropy = PPOAGENT.calAObj(\n",
    "        CopyAgent,\n",
    "        state,\n",
    "        action,\n",
    "        gT.detach() - critic.detach()\n",
    "    )\n",
    "    minusObj.backward()\n",
    "    obj = minusObj.cpu().sum().detach().numpy()\n",
    "    lossC = lossC.cpu().sum().detach().numpy()\n",
    "\n",
    "    if writeMode:\n",
    "        writer.add_scalar(\"Obj\", -obj, _step+_epoch)\n",
    "        writer.add_scalar(\"Critic Lostt\", lossC, _step+_epoch)\n",
    "        entropy = entropy.detach().cpu().numpy()\n",
    "        writer.add_scalar(\"Entropy\", entropy, _step + _epoch)\n",
    "\n",
    "        gT = gT.view(-1)\n",
    "        gT = torch.mean(gT).detach().cpu().numpy()\n",
    "        writer.add_scalar(\"gT\", gT, _step + _epoch)\n",
    "\n",
    "        critic = critic.view(-1)\n",
    "        critic = torch.mean(critic).detach().cpu().numpy()\n",
    "        writer.add_scalar(\"critic\", critic, _step + _epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReturn(\n",
    "    reward,\n",
    "    critic,\n",
    "    nCritic,\n",
    "    done\n",
    ")->tuple:\n",
    "    gT, gAE = [], []\n",
    "    length = len(reward)\n",
    "    critic = critic.view(length, -1)\n",
    "    nCritic = nCritic.view(length, -1)\n",
    "    for i in range(TotalAgent):\n",
    "        rA = reward[:, i]  # [step] , 100\n",
    "        dA = done[:, i]  # [step] , 100\n",
    "        cA = critic[:, i]\n",
    "        ncA = nCritic[:, i] \n",
    "        GT = []\n",
    "        GTDE = []\n",
    "        discounted_Td = 0\n",
    "        if dA[-1]:\n",
    "            discounted_r = cA[-1]\n",
    "        else:\n",
    "            discounted_r = ncA[-1]\n",
    "\n",
    "        for r, is_terminal, c, nc in zip(\n",
    "                reversed(rA), \n",
    "                reversed(dA), \n",
    "                reversed(cA),\n",
    "                reversed(ncA)):\n",
    "            \n",
    "            if is_terminal:\n",
    "                td_error = r + gamma * c - c\n",
    "            else:\n",
    "                td_error = r + gamma * nc - c\n",
    "\n",
    "            discounted_r = r + gamma * discounted_r\n",
    "            discounted_Td = td_error + gamma * lambda_ * discounted_Td\n",
    "\n",
    "            GT.append(discounted_r)\n",
    "            GTDE.append(discounted_Td)\n",
    "        GT = torch.tensor(GT[::-1]).view((-1, 1)).to(device)\n",
    "        GTDE = torch.tensor(GTDE[::-1]).view((-1, 1)).to(device)\n",
    "        gT.append(GT)\n",
    "        gAE.append(GTDE)\n",
    "\n",
    "    gT = torch.cat(gT, dim=0)\n",
    "    gAE = torch.cat(gAE, dim=0)\n",
    "\n",
    "    gT = gT.view(TotalAgent, -1)\n",
    "    gT = gT.permute(1, 0).contiguous()\n",
    "    gT = gT.view((-1, 1))\n",
    "\n",
    "    gAE = gAE.view(TotalAgent, -1)\n",
    "    gAE = gAE.permute(1, 0).contiguous()\n",
    "    gAE = gAE.view((-1, 1)) \n",
    "    # seq, agent > seq * agent\n",
    "\n",
    "    return gT, gAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepGradient(_step, _epoch):\n",
    "    Agent.critic.clippingNorm(ClipingNormCritic)\n",
    "    cOptim.step()\n",
    "    Agent.actor.clippingNorm(ClipingNormActor)\n",
    "    aOptim.step()\n",
    "\n",
    "    normA = Agent.actor.calculateNorm().cpu().detach().numpy()\n",
    "    normC = Agent.critic.calculateNorm().cpu().detach().numpy()\n",
    "    if writeMode:\n",
    "        writer.add_scalar('Action Gradient Mag', normA, _step+_epoch)\n",
    "        writer.add_scalar('Critic Gradient Mag', normC, _step+_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessBatch(_step, _epoch):\n",
    "    k1 = 160\n",
    "    k2 = 10\n",
    "    div = int(k1/k2)\n",
    "    rstate, lidarPt, action, reward, done = \\\n",
    "        [], [], [], [], []\n",
    "    num_list = int(len(ReplayMemory_Trajectory)/k1)\n",
    "    trstate, tlidarPt = [[] for __ in range(num_list)], [[] for _ in range(num_list)]\n",
    "    tState = [[] for _ in range(num_list - 1)]\n",
    "    for ss in ReplayMemory:\n",
    "        s, a, r, ns, d = ss\n",
    "        rstate.append(s[0])\n",
    "        lidarPt.append(s[1])\n",
    "        action.append(a)\n",
    "        reward.append(r)\n",
    "        done.append(d)\n",
    "    z = 0\n",
    "    for ts in ReplayMemory_Trajectory:\n",
    "        trstate[int(z/k1)].append(ts[0])\n",
    "        tlidarPt[int(z/k1)].append(ts[1])\n",
    "        z+=1\n",
    "    if len(trstate) == k1:\n",
    "        zeroMode = True\n",
    "    else:\n",
    "        for _ in range(num_list - 1):\n",
    "            # print(trstate[_])\n",
    "            tState[_] = (torch.cat(trstate[_], dim=0), torch.cat(tlidarPt[_], dim=0))\n",
    "        zeroMode = False\n",
    "    rstate = torch.cat(rstate, dim=0)\n",
    "    lidarPt = torch.cat(lidarPt, dim=0)\n",
    "    nrstate, nlidarPt = ns\n",
    "    nrstate, nlidarPt = torch.cat((rstate, nrstate), dim=0), torch.cat((lidarPt, nlidarPt), dim=0)\n",
    "    lidarPt = lidarPt.view((-1, TotalAgent, 1, sSize[-1]))\n",
    "    rstate = rstate.view((-1, TotalAgent, 6))\n",
    "\n",
    "    nstate = (nrstate, nlidarPt)\n",
    "\n",
    "    reward = np.array(reward)\n",
    "    done = np.array(done)\n",
    "    action = torch.tensor(action).to(device)\n",
    "\n",
    "    Agent.actor.zeroCellState()\n",
    "    Agent.critic.zeroCellState()\n",
    "    CopyAgent.actor.zeroCellState()\n",
    "    CopyAgent.critic.zeroCellState()\n",
    "\n",
    "    if zeroMode is False:\n",
    "        with torch.no_grad():\n",
    "            for tr in tState:\n",
    "                tr_cuda = tuple([x.to(device) for x in tr])\n",
    "                Agent.critic.forward(tr_cuda)\n",
    "                Agent.actor.forward(tr_cuda)\n",
    "                CopyAgent.critic.forward(tr_cuda)\n",
    "                CopyAgent.actor.forward(tr_cuda)\n",
    "                del tr_cuda\n",
    "            Agent.actor.detachCellState()\n",
    "            Agent.critic.detachCellState()\n",
    "            CopyAgent.actor.detachCellState()\n",
    "            CopyAgent.critic.detachCellState()\n",
    "    \n",
    "    InitActorCellState = Agent.actor.getCellState()\n",
    "    InitCopyActorCellState = CopyAgent.actor.getCellState()\n",
    "\n",
    "    InitCriticCellState = CopyAgent.actor.getCellState()\n",
    "    InitCopyCriticCellState = CopyAgent.critic.getCellState()\n",
    "    zeroGrad()\n",
    "\n",
    "    for _ in range(epoch):\n",
    "        Agent.actor.setCellState(InitActorCellState)\n",
    "        Agent.critic.setCellState(InitCriticCellState)\n",
    "\n",
    "        value = Agent.critic.forward(nstate)[0]\n",
    "        value = value.view(k1+1, TotalAgent, 1)\n",
    "        nvalue = value[1:]\n",
    "        value = value[:-1]\n",
    "        gT, gAE = getReturn(reward, value, nvalue, done)\n",
    "        gT = gT.view(k1, TotalAgent)\n",
    "        gAE = gAE.view(k1, TotalAgent)\n",
    "\n",
    "        Agent.critic.setCellState(InitCriticCellState)\n",
    "        CopyAgent.actor.setCellState(InitCopyActorCellState)\n",
    "        CopyAgent.critic.setCellState(InitCopyCriticCellState)\n",
    "        for i in range(div):\n",
    "            _rstate = rstate[i*k2:(i+1)*k2].view(-1, 6)\n",
    "            _lidarpt = lidarPt[i*k2:(1+i)*k2].view(-1, 1, sSize[-1])\n",
    "            _state = (_rstate, _lidarpt)\n",
    "            _action = action[i*k2:(i+1)*k2].view((-1, 2))\n",
    "            _gT = gT[i*k2:(i+1)*k2].view(-1, 1)\n",
    "            _gAE = gAE[i*k2:(i+1)*k2].view(-1, 1)\n",
    "            _value = value[i*k2:(i+1)*k2].view(-1, 1)\n",
    "            train(Agent, _state, _action, _gT, _gAE, _value, _step, _epoch)\n",
    "            Agent.actor.detachCellState()\n",
    "            Agent.critic.detachCellState()\n",
    "        stepGradient(_step+i, _epoch)\n",
    "        Agent.actor.zeroCellState()\n",
    "        Agent.critic.zeroCellState()\n",
    "        zeroGrad()\n",
    "        if zeroMode is False:\n",
    "            with torch.no_grad():\n",
    "                for tr in tState:\n",
    "                    tr_cuda = tuple([x.to(device) for x in tr])\n",
    "                    Agent.critic.forward(tr_cuda)\n",
    "                    Agent.actor.forward(tr_cuda)\n",
    "                    del tr_cuda\n",
    "                Agent.critic.detachCellState()\n",
    "                Agent.actor.detachCellState()\n",
    "        InitActorCellState = Agent.actor.getCellState()\n",
    "        InitCriticCellState = Agent.critic.getCellState()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "InferenceTime:0.136\n",
      "InferenceTime:0.080\n",
      "InferenceTime:0.069\n",
      "InferenceTime:0.069\n",
      "InferenceTime:0.079\n",
      "InferenceTime:0.086\n",
      "InferenceTime:0.089\n",
      "InferenceTime:0.077\n",
      "InferenceTime:0.070\n",
      "InferenceTime:0.095\n",
      "InferenceTime:0.088\n",
      "InferenceTime:0.074\n",
      "InferenceTime:0.077\n",
      "InferenceTime:0.080\n",
      "InferenceTime:0.074\n",
      "InferenceTime:0.079\n",
      "InferenceTime:0.088\n",
      "InferenceTime:0.082\n",
      "InferenceTime:0.080\n",
      "InferenceTime:0.074\n",
      "InferenceTime:0.073\n",
      "InferenceTime:0.071\n",
      "InferenceTime:0.077\n",
      "InferenceTime:0.079\n",
      "InferenceTime:0.084\n",
      "InferenceTime:0.072\n",
      "InferenceTime:0.084\n",
      "InferenceTime:0.073\n",
      "InferenceTime:0.079\n",
      "InferenceTime:0.070\n",
      "InferenceTime:0.094\n",
      "InferenceTime:0.083\n",
      "InferenceTime:0.079\n",
      "InferenceTime:0.079\n",
      "InferenceTime:0.074\n",
      "InferenceTime:0.107\n",
      "InferenceTime:0.079\n",
      "InferenceTime:0.079\n",
      "InferenceTime:0.073\n",
      "InferenceTime:0.082\n",
      "InferenceTime:0.083\n",
      "InferenceTime:0.081\n",
      "InferenceTime:0.076\n",
      "InferenceTime:0.074\n",
      "InferenceTime:0.085\n",
      "InferenceTime:0.072\n",
      "InferenceTime:0.074\n",
      "InferenceTime:0.070\n",
      "InferenceTime:0.075\n",
      "InferenceTime:0.072\n",
      "InferenceTime:0.081\n",
      "InferenceTime:0.081\n",
      "InferenceTime:0.071\n",
      "InferenceTime:0.070\n",
      "InferenceTime:0.074\n",
      "InferenceTime:0.072\n",
      "InferenceTime:0.070\n",
      "InferenceTime:0.071\n",
      "InferenceTime:0.107\n",
      "InferenceTime:0.071\n",
      "InferenceTime:0.074\n",
      "InferenceTime:0.102\n",
      "InferenceTime:0.081\n",
      "InferenceTime:0.079\n",
      "InferenceTime:0.074\n",
      "InferenceTime:0.069\n",
      "InferenceTime:0.072\n",
      "InferenceTime:0.075\n",
      "InferenceTime:0.072\n",
      "InferenceTime:0.080\n",
      "InferenceTime:0.074\n",
      "InferenceTime:0.078\n",
      "InferenceTime:0.079\n",
      "InferenceTime:0.070\n",
      "InferenceTime:0.075\n",
      "InferenceTime:0.092\n",
      "InferenceTime:0.070\n",
      "InferenceTime:0.075\n",
      "InferenceTime:0.085\n",
      "InferenceTime:0.078\n",
      "InferenceTime:0.089\n",
      "InferenceTime:0.085\n",
      "InferenceTime:0.080\n",
      "InferenceTime:0.080\n",
      "InferenceTime:0.083\n",
      "InferenceTime:0.076\n",
      "InferenceTime:0.081\n",
      "InferenceTime:0.129\n",
      "InferenceTime:0.076\n",
      "InferenceTime:0.074\n",
      "InferenceTime:0.086\n",
      "InferenceTime:0.081\n",
      "InferenceTime:0.077\n",
      "InferenceTime:0.073\n",
      "InferenceTime:0.088\n",
      "InferenceTime:0.072\n",
      "InferenceTime:0.080\n",
      "InferenceTime:0.073\n",
      "InferenceTime:0.074\n",
      "InferenceTime:0.073\n",
      "InferenceTime:0.113\n",
      "InferenceTime:0.077\n",
      "InferenceTime:0.076\n",
      "InferenceTime:0.073\n",
      "InferenceTime:0.078\n",
      "InferenceTime:0.073\n",
      "InferenceTime:0.077\n",
      "InferenceTime:0.080\n",
      "InferenceTime:0.078\n",
      "InferenceTime:0.077\n",
      "InferenceTime:0.076\n",
      "InferenceTime:0.075\n",
      "InferenceTime:0.074\n",
      "InferenceTime:0.105\n",
      "InferenceTime:0.075\n",
      "InferenceTime:0.079\n",
      "InferenceTime:0.080\n",
      "InferenceTime:0.074\n",
      "InferenceTime:0.087\n",
      "InferenceTime:0.072\n",
      "InferenceTime:0.085\n",
      "InferenceTime:0.069\n",
      "InferenceTime:0.070\n",
      "InferenceTime:0.071\n",
      "InferenceTime:0.073\n",
      "InferenceTime:0.070\n",
      "InferenceTime:0.131\n",
      "InferenceTime:0.078\n",
      "InferenceTime:0.075\n",
      "InferenceTime:0.076\n",
      "InferenceTime:0.084\n",
      "InferenceTime:0.082\n",
      "InferenceTime:0.082\n",
      "InferenceTime:0.074\n",
      "InferenceTime:0.070\n",
      "InferenceTime:0.085\n",
      "InferenceTime:0.109\n",
      "InferenceTime:0.077\n",
      "InferenceTime:0.093\n",
      "InferenceTime:0.084\n",
      "InferenceTime:0.087\n",
      "InferenceTime:0.086\n",
      "InferenceTime:0.083\n",
      "InferenceTime:0.075\n",
      "InferenceTime:0.078\n",
      "InferenceTime:0.084\n",
      "InferenceTime:0.079\n",
      "InferenceTime:0.084\n",
      "InferenceTime:0.081\n",
      "InferenceTime:0.085\n",
      "InferenceTime:0.128\n",
      "InferenceTime:0.082\n",
      "InferenceTime:0.081\n",
      "InferenceTime:0.083\n",
      "InferenceTime:0.090\n",
      "InferenceTime:0.082\n",
      "InferenceTime:0.079\n",
      "InferenceTime:0.087\n",
      "InferenceTime:0.074\n",
      "InferenceTime:0.087\n",
      "InferenceTime:0.086\n",
      "InferenceTime:0.075\n",
      "InferenceTime:0.085\n",
      "InferenceTime:0.074\n",
      "InferenceTime:0.072\n",
      "InferenceTime:0.079\n",
      "InferenceTime:0.077\n",
      "InferenceTime:0.082\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/obj_detect/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mcurrentframe\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1493\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mframelist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mcurrentframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1496\u001b[0m     \u001b[0;34m\"\"\"Return the frame of the caller or None if this is not possible.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_getframe\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while 1:\n",
    "    stateT, action = initSampling()\n",
    "    done = Sampling(stateT, action)\n",
    "    # preprocessBatch(step, epoch)\n",
    "    ReplayMemory.clear()\n",
    "    if True in done:\n",
    "        for e in envs:\n",
    "            Agent.actor.zeroCellState()\n",
    "            Agent.critic.zeroCellState()\n",
    "            OldAgent.actor.zeroCellState()\n",
    "            OldAgent.critic.zeroCellState()\n",
    "            CopyAgent.actor.zeroCellState()\n",
    "            CopyAgent.critic.zeroCellState()\n",
    "            ReplayMemory_Trajectory.clear()\n",
    "            e.step.remote()\n",
    "    OldAgent.update(Agent)\n",
    "    CopyAgent.update(Agent)\n",
    "\n",
    "    if step % 4000 == 0:\n",
    "        _reward = Rewards.mean()\n",
    "        if writeMode:\n",
    "            writer.add_scalar('Performance', _reward, step)\n",
    "        print(\"\"\"\n",
    "        Step : {:5d} // Performance : {:.3f}\n",
    "        \"\"\".format(step, _reward))\n",
    "        Rewards = np.zeros(TotalAgent)\n",
    "        torch.save(Agent.state_dict(), sPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}